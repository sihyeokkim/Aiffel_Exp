{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e995b1",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Module-import\" data-toc-modified-id=\"Module-import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Module import</a></span></li><li><span><a href=\"#데이터-확인\" data-toc-modified-id=\"데이터-확인-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>데이터 확인</a></span></li><li><span><a href=\"#전처리\" data-toc-modified-id=\"전처리-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>전처리</a></span></li><li><span><a href=\"#데이터정제\" data-toc-modified-id=\"데이터정제-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>데이터정제</a></span></li><li><span><a href=\"#Tokenizing\" data-toc-modified-id=\"Tokenizing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Tokenizing</a></span></li><li><span><a href=\"#데이터-분리\" data-toc-modified-id=\"데이터-분리-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>데이터 분리</a></span><ul class=\"toc-item\"><li><span><a href=\"#src_input,-tgt_input-생성\" data-toc-modified-id=\"src_input,-tgt_input-생성-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>src_input, tgt_input 생성</a></span></li><li><span><a href=\"#training/test-set-분리\" data-toc-modified-id=\"training/test-set-분리-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>training/test set 분리</a></span></li></ul></li><li><span><a href=\"#데이터셋-객체생성\" data-toc-modified-id=\"데이터셋-객체생성-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>데이터셋 객체생성</a></span></li><li><span><a href=\"#학습시키기\" data-toc-modified-id=\"학습시키기-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>학습시키기</a></span></li><li><span><a href=\"#손실값-시각화\" data-toc-modified-id=\"손실값-시각화-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>손실값 시각화</a></span></li><li><span><a href=\"#가사생성\" data-toc-modified-id=\"가사생성-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>가사생성</a></span><ul class=\"toc-item\"><li><span><a href=\"#실행\" data-toc-modified-id=\"실행-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>실행</a></span></li></ul></li><li><span><a href=\"#회고\" data-toc-modified-id=\"회고-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>회고</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31684a",
   "metadata": {},
   "source": [
    "# Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "101f841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 180847\n",
      "Examples:\n",
      " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('homepath')+'/aiffel/lyricist/data/lyrics/*' \n",
    "'''os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. \n",
    "   txt_file_path 에 \"/root/aiffel/lyricist/data/lyrics/*\" 저장\n",
    "'''\n",
    "txt_list = glob.glob(txt_file_path) \n",
    "'''txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
    "'''\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d805c1c",
   "metadata": {},
   "source": [
    "# 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb1455d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for some education\n",
      "Made my way into the night\n",
      "All that bullshit conversation\n",
      "Baby, can't you read the signs? I won't bore you with the details, baby\n",
      "I don't even wanna waste your time\n",
      "Let's just say that maybe\n",
      "You could help me ease my mind\n",
      "I ain't Mr. Right But if you're looking for fast love\n",
      "If that's love in your eyes\n",
      "It's more than enough\n",
      "Had some bad love\n"
     ]
    }
   ],
   "source": [
    "# enumerate() 함수를 이용하여 raw_corpus list 내에 저장된 문장과 그 문장의 인덱스를 반환 (인덱스, 문장 순)\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다. 문장을 line별로 가져움을 상기!!\n",
    "\n",
    "    if idx > 10: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab8a3",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dae9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5251b",
   "metadata": {},
   "source": [
    "\n",
    "# 데이터정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5d9807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> looking for some education <end>',\n",
       " '<start> made my way into the night <end>',\n",
       " '<start> all that bullshit conversation <end>',\n",
       " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
       " '<start> i don t even wanna waste your time <end>',\n",
       " '<start> let s just say that maybe <end>',\n",
       " '<start> you could help me ease my mind <end>',\n",
       " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
       " '<start> if that s love in your eyes <end>',\n",
       " '<start> it s more than enough <end>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    #if len(sentence) > 15:  continue\n",
    "    \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98228532",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "771257fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 311  28 ...   0   0   0]\n",
      " [  2 219  13 ...   0   0   0]\n",
      " [  2  24  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  30  21 ...   1   3   0]\n",
      " [  2 336  21 ...   3   0   0]\n",
      " [  2  42 131 ...   0   0   0]] <keras.preprocessing.text.Tokenizer object at 0x000001F5427CDA60>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화는 문장을 숫자형태로 변환하는 개념. 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 12000단어를 기억할 수 있는 tokenizer\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없음\n",
    "    # 14000단어에 포함되지 못한 단어는 '<unk>'로 \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=14000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "\n",
    "    # max tocken len = 15로 정함\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "250666eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169526, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97b1ad",
   "metadata": {},
   "source": [
    "# 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95734f",
   "metadata": {},
   "source": [
    "## src_input, tgt_input 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99c64132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  311   28   98 4740    3    0    0    0    0    0    0    0    0]\n",
      "(169526, 14)\n",
      "[ 311   28   98 4740    3    0    0    0    0    0    0    0    0    0]\n",
      "(169526, 14)\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:] #<start = 0번째 index>    \n",
    "\n",
    "print(src_input[0])\n",
    "print(src_input.shape)\n",
    "print(tgt_input[0])\n",
    "print(tgt_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb47b0",
   "metadata": {},
   "source": [
    "## training/test set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dd40c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135620, 14) (33906, 14)\n",
      "(135620, 14) (33906, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=15)\n",
    "\n",
    "print(enc_train.shape, enc_val.shape)\n",
    "print(dec_train.shape, dec_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e66b80",
   "metadata": {},
   "source": [
    "# 데이터셋 객체생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "142dd077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "'''tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "  tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n",
    "  tokenize() 함수에서 num_words를 12000개로 선언했기 때문에, tokenizer.num_words의 값은 000\n",
    "\n",
    "  뒤섞기,이 함수의 parameter 중 하나는 buffer_size, dataset이 올바른 방법으로 실제로 섞이지 않기 때문에\n",
    "  Training성능이 낮게될 것임, 따라서 trainig을 시작할 때, dataset은 BUFFER_SIZE 만큼 가져오고 \n",
    "  다음 램덤하게 BATCH_SIZE만큼 PICK할 것임 '''\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7be03",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7dc3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
    "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
    "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
    "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져옵니다!   \n",
    "embedding_size = 256 # 벡터의 차원수\n",
    "hidden_size = 1024\n",
    "\n",
    "#언어 모델(Languagel Model)이란 단어 시퀀스(문장)에 확률을 할당하는 모델\n",
    "#어떤 문장들이 있을 때, 기계가 이 문장은 적절해! 이 문장은 말이 안 돼! 라고 \n",
    "#사람처럼 정확히 판단할 수 있다면, 기계이 자연어 처리의 성능이 뛰어나다고 말할 수 있습니다.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) \n",
    "# tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7b2b543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 14001), dtype=float32, numpy=\n",
       "array([[[ 2.39055938e-04, -7.00694218e-05,  1.04391365e-05, ...,\n",
       "         -5.17294757e-06,  1.58298557e-04, -2.08667298e-05],\n",
       "        [ 1.72917455e-04, -1.86723133e-04,  9.60677426e-06, ...,\n",
       "          3.20626932e-05,  2.57108419e-04, -1.76653703e-05],\n",
       "        [ 8.24528106e-05, -2.33285900e-05,  1.78741771e-04, ...,\n",
       "         -3.39577309e-06,  2.67646508e-04, -7.79754919e-05],\n",
       "        ...,\n",
       "        [-3.38219543e-05, -1.78667076e-03,  1.04215508e-03, ...,\n",
       "          1.61900336e-03, -1.15865532e-06,  1.96250563e-04],\n",
       "        [-1.30233544e-04, -2.11479585e-03,  1.04573893e-03, ...,\n",
       "          1.97328161e-03, -6.37912817e-05,  3.72149196e-04],\n",
       "        [-2.58939603e-04, -2.42630648e-03,  1.02163677e-03, ...,\n",
       "          2.31066043e-03, -1.08652850e-04,  4.99248970e-04]],\n",
       "\n",
       "       [[ 2.39055938e-04, -7.00694218e-05,  1.04391365e-05, ...,\n",
       "         -5.17294757e-06,  1.58298557e-04, -2.08667298e-05],\n",
       "        [ 2.13907653e-04,  6.45014006e-05,  1.77265829e-04, ...,\n",
       "         -3.03954468e-04, -1.37597672e-05, -2.33446772e-04],\n",
       "        [ 1.23545658e-04,  2.12519837e-04,  1.15053030e-04, ...,\n",
       "         -4.22909739e-04, -1.07017375e-04, -3.09936062e-04],\n",
       "        ...,\n",
       "        [-1.09775974e-04, -2.17872206e-03,  1.02730817e-03, ...,\n",
       "          2.61441176e-03, -2.78546737e-04,  4.31042572e-04],\n",
       "        [-3.19933548e-04, -2.46844604e-03,  1.00515282e-03, ...,\n",
       "          2.89626536e-03, -2.41463422e-04,  4.55433561e-04],\n",
       "        [-5.29158860e-04, -2.71388888e-03,  9.79880686e-04, ...,\n",
       "          3.13991145e-03, -1.95944856e-04,  4.70933097e-04]],\n",
       "\n",
       "       [[ 2.39055938e-04, -7.00694218e-05,  1.04391365e-05, ...,\n",
       "         -5.17294757e-06,  1.58298557e-04, -2.08667298e-05],\n",
       "        [ 2.75327533e-04, -1.62672513e-04,  2.86659779e-05, ...,\n",
       "         -1.04623818e-04,  2.72458157e-04,  2.57377833e-05],\n",
       "        [ 1.88009406e-04,  3.34307551e-05,  3.52499774e-05, ...,\n",
       "         -1.80101313e-04,  4.22343233e-04, -3.08938834e-05],\n",
       "        ...,\n",
       "        [-2.95574253e-04, -2.12994730e-03,  1.04197254e-03, ...,\n",
       "          1.98233267e-03,  3.44380562e-04,  1.90460210e-04],\n",
       "        [-4.52807086e-04, -2.46531470e-03,  1.02191069e-03, ...,\n",
       "          2.31678411e-03,  3.14647332e-04,  2.37904518e-04],\n",
       "        [-6.13486103e-04, -2.75084237e-03,  9.93474037e-04, ...,\n",
       "          2.62114033e-03,  2.91853357e-04,  2.76014209e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.39055938e-04, -7.00694218e-05,  1.04391365e-05, ...,\n",
       "         -5.17294757e-06,  1.58298557e-04, -2.08667298e-05],\n",
       "        [ 3.87994369e-04, -1.22793543e-04,  5.03455522e-06, ...,\n",
       "          1.78646180e-04, -1.23577760e-04, -1.16130926e-04],\n",
       "        [ 4.85523982e-04,  4.08283377e-05,  1.79922616e-04, ...,\n",
       "          4.25775797e-04, -6.11257914e-04, -1.13626142e-04],\n",
       "        ...,\n",
       "        [-1.27509783e-03, -5.06953220e-04,  9.78712458e-04, ...,\n",
       "          2.34208041e-04, -1.50297396e-03, -2.96715501e-04],\n",
       "        [-1.39878073e-03, -5.57736435e-04,  6.35443663e-04, ...,\n",
       "          2.20374641e-04, -1.04525918e-03, -3.99962591e-04],\n",
       "        [-1.33958901e-03, -6.91413064e-04,  3.46068875e-04, ...,\n",
       "          1.34753209e-04, -6.34498894e-04, -3.65702348e-04]],\n",
       "\n",
       "       [[ 2.39055938e-04, -7.00694218e-05,  1.04391365e-05, ...,\n",
       "         -5.17294757e-06,  1.58298557e-04, -2.08667298e-05],\n",
       "        [ 4.14201524e-04,  1.30766915e-04,  1.72819753e-04, ...,\n",
       "          1.83089578e-04,  1.86766294e-04, -1.20300698e-04],\n",
       "        [ 5.40427747e-04,  4.75174049e-04,  4.64658398e-04, ...,\n",
       "          2.13497609e-04,  1.48499559e-04, -1.06841122e-04],\n",
       "        ...,\n",
       "        [ 1.00104895e-03, -9.42664105e-04,  2.16757669e-03, ...,\n",
       "          1.64312590e-03, -5.02024486e-05,  1.99642556e-04],\n",
       "        [ 7.69096659e-04, -1.43099588e-03,  2.02196208e-03, ...,\n",
       "          1.99363125e-03, -7.94613443e-05,  2.86699389e-04],\n",
       "        [ 5.14749205e-04, -1.87120750e-03,  1.85997272e-03, ...,\n",
       "          2.32650340e-03, -9.63279599e-05,  3.52760690e-04]],\n",
       "\n",
       "       [[ 2.39055938e-04, -7.00694218e-05,  1.04391365e-05, ...,\n",
       "         -5.17294757e-06,  1.58298557e-04, -2.08667298e-05],\n",
       "        [ 3.86517262e-04, -2.26917109e-04,  2.13428124e-04, ...,\n",
       "         -9.59481258e-06,  3.47389374e-04,  5.43102651e-05],\n",
       "        [ 3.52319272e-04, -3.46275454e-04,  4.26216575e-04, ...,\n",
       "          2.65190407e-04,  5.73916594e-04,  3.14768113e-04],\n",
       "        ...,\n",
       "        [ 1.07913744e-04, -3.68103792e-05,  6.77720876e-04, ...,\n",
       "          4.59502044e-05,  2.86493130e-04, -2.26438598e-04],\n",
       "        [ 3.50031361e-04, -3.83213686e-04,  8.08822340e-04, ...,\n",
       "          3.59861820e-04,  3.00110027e-04, -2.85567949e-05],\n",
       "        [ 5.02880604e-04, -8.00535083e-04,  9.26629989e-04, ...,\n",
       "          7.30994274e-04,  2.92296434e-04,  1.75916648e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc37c00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#언어 모델(Languagel Model)이란 단어 시퀀스(문장)에 확률을 할당하는 모델\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97089cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529/529 [==============================] - 1827s 3s/step - loss: 5.6675 - accuracy: 0.4233 - val_loss: 4.4719 - val_accuracy: 0.4356\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.1) \n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, # 기본값은 False이다. 모델에 의해 생성된 출력 값이 정규화되지 않았음을 손실 함수에 알려준다. 즉 softmax함수가 적용되지 않았다는걸 의미한다. \n",
    "    reduction='none'  # 기본값은 SUM이다. 각자 나오는 값의 반환 원할 때 None을 사용한다.\n",
    ")\n",
    "# 모델을 학습시키키 위한 학습과정을 설정하는 단계\n",
    "model.compile(loss=loss, optimizer=optimizer,  metrics=['accuracy']) # 손실함수와 훈련과정을 설정\n",
    "history = model.fit(dataset,\n",
    "          epochs=1,\n",
    "          validation_data=val_dataset,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "209b2e41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_chief_worker_only',\n",
       " '_implements_predict_batch_hooks',\n",
       " '_implements_test_batch_hooks',\n",
       " '_implements_train_batch_hooks',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " '_supports_tf_logs',\n",
       " 'epoch',\n",
       " 'history',\n",
       " 'model',\n",
       " 'on_batch_begin',\n",
       " 'on_batch_end',\n",
       " 'on_epoch_begin',\n",
       " 'on_epoch_end',\n",
       " 'on_predict_batch_begin',\n",
       " 'on_predict_batch_end',\n",
       " 'on_predict_begin',\n",
       " 'on_predict_end',\n",
       " 'on_test_batch_begin',\n",
       " 'on_test_batch_end',\n",
       " 'on_test_begin',\n",
       " 'on_test_end',\n",
       " 'on_train_batch_begin',\n",
       " 'on_train_batch_end',\n",
       " 'on_train_begin',\n",
       " 'on_train_end',\n",
       " 'params',\n",
       " 'set_model',\n",
       " 'set_params',\n",
       " 'validation_data']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "179a055c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3583a3e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [5.667545318603516],\n",
       " 'accuracy': [0.4233476221561432],\n",
       " 'val_loss': [4.471937656402588],\n",
       " 'val_accuracy': [0.43560606241226196]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265a980",
   "metadata": {},
   "source": [
    "# 손실값 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "558e4643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 훈련 과정 시각화 (정확도)\n",
    "fig = plt.figure(figsize=(13, 7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim(0, 2)\n",
    "plt.ylim(0, 10)\n",
    "plt.legend(['Train', 'Test' ], loc='lower right')\n",
    "#plt.show()\n",
    "\n",
    "# 훈련 과정 시각화 (손실)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlim(0, 2)\n",
    "plt.ylim(0, 10)\n",
    "plt.legend(['Train', 'Test' ], loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599a763",
   "metadata": {},
   "source": [
    "# 가사생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "998fda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
    "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성\n",
    "        # 1 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict = model(test_tensor) # \n",
    "        # 2 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 우리 모델이 를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d2eeb",
   "metadata": {},
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9d236fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i ve <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\" <start> i \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1436eb1",
   "metadata": {},
   "source": [
    "# 회고\n",
    "    Case1) local jupyter 활용\n",
    "        embedding_size = 256 \n",
    "        hidden_size = 1024\n",
    "        epochs=1 <컴퓨터가 터질 것 같아 안전상 여기에서 멈춤, safety first!>\n",
    "        Total params: 31,574,961\n",
    "        val_loss: 4.471937656402588 \n",
    "        generate_text()실행시 창의성이 너~~무 낮았다.  i 뒤에 ve 이라니..\n",
    "        embedding_size 및 hidden_size를 늘려 추상화를 늘리고 학습회수도 조금 더 조정해야 \n",
    "        창의성이 발휘 될 수 있을 것 같습니다. \n",
    "\n",
    "    \n",
    "    Case2) LMS에서 동일한 코드를 적용 후 결과\n",
    "        embedding_size = 2048 \n",
    "        hidden_size = 2048\n",
    "        epochs=10\n",
    "        val_loss: 2.2621097564697266\n",
    "        Total params: 124,487,345\n",
    "        입력문자: '<start> feel\n",
    "        생성문장: '<start> feel like i m livin at the bottom of a grave <end> '\n",
    "        입력문자: '<start> oh\n",
    "        생성문자: '<start> ohh , i , i feel like going on , yes i do <end> '\n",
    "        입력문자: '<start> sky\n",
    "        생성문자: '<start> sky is the limit and you know that you keep on <end> '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1babd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "827.778px",
    "left": "22px",
    "top": "178.944px",
    "width": "255.984px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "709.844px",
    "left": "1194px",
    "right": "20px",
    "top": "120px",
    "width": "466px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
